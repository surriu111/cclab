{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/surriu111/cclab/blob/main/attention%2Bperception_loss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxlFhpLKmRr6",
        "outputId": "3d5485e7-566a-4649-9f9b-a24e9e15ad35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['types2label_128_pruned7.txt',\n",
              " 'train_labels_128.npy',\n",
              " 'demo.ipynb',\n",
              " 'label2type.txt',\n",
              " 'model.py',\n",
              " 'train.npz',\n",
              " 'Untitled folder']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# Mount to Google Drive & Switch to the dataset directory\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "\n",
        "os.listdir('/content/drive/MyDrive/ml-com')\n",
        "\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "UC-UirCtmBse"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Set a fixed random seed for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jJz848nmCJ5",
        "outputId": "89328f81-073c-4871-e53a-530218195ea7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ml-com/train.npz: images shape (18900, 3, 128, 128), labels shape (18900,)\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, npz_path):\n",
        "        npz_data = np.load(npz_path)\n",
        "        self.images = npz_data[\"images\"] # (N, 3, 128, 128) in np.uint8\n",
        "        self.labels = npz_data[\"labels\"] # (N,) in np.int64\n",
        "        assert self.images.shape[0] == self.labels.shape[0]\n",
        "        print(f\"{npz_path}: images shape {self.images.shape}, \"\n",
        "              f\"labels shape {self.labels.shape}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = torch.tensor(self.images[idx]) / 255 # convert to [0, 1] range\n",
        "        label = torch.tensor(self.labels[idx])\n",
        "        return image, label\n",
        "\n",
        "npz_path = '/content/drive/MyDrive/ml-com/train.npz'\n",
        "train_dataset = CustomDataset(npz_path)\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oypYyKY0p17X",
        "outputId": "fb80a356-d290-48ab-c4ed-d2e36d406b30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "images shape: torch.Size([128, 3, 128, 128])\n",
            "labels shape: torch.Size([128])\n"
          ]
        }
      ],
      "source": [
        "# sample data batch\n",
        "images, labels = next(iter(train_loader))\n",
        "print(f\"images shape: {images.shape}\")\n",
        "print(f\"labels shape: {labels.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "f6sRs_EfuDR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.query = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
        "        self.key = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
        "        self.value = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.size()\n",
        "        q = self.query(x).view(B, -1, H * W).permute(0, 2, 1)   # B x HW x C'\n",
        "        k = self.key(x).view(B, -1, H * W)                      # B x C' x HW\n",
        "        attn = torch.bmm(q, k)                                 # B x HW x HW\n",
        "        attn = torch.softmax(attn, dim=-1)\n",
        "        v = self.value(x).view(B, -1, H * W)                   # B x C x HW\n",
        "        out = torch.bmm(v, attn.permute(0, 2, 1))              # B x C x HW\n",
        "        out = out.view(B, C, H, W)\n",
        "        return self.gamma * out + x\n"
      ],
      "metadata": {
        "id": "PwUG0JCudsYm"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self, latent_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(latent_dim * 32 * 32, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.fc(z)\n"
      ],
      "metadata": {
        "id": "Dw9Q0vcHI1rr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvVAE(nn.Module):\n",
        "    def __init__(self, input_channels=3, latent_channels=8):\n",
        "        super().__init__()\n",
        "\n",
        "        # --- Encoder ---\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, 64, kernel_size=3, stride=2, padding=1),  # 64 x 64 x 64\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),             # 128 x 32 x 32\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),            # 256 x 32 x 32\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # --- Attention ---\n",
        "        self.attn = SelfAttention(256)\n",
        "\n",
        "        # --- Quantization conv ---\n",
        "        self.quant_conv = nn.Conv2d(256, latent_channels * 2, kernel_size=1)    # 4+4 channels\n",
        "\n",
        "        # --- Decoder ---\n",
        "        self.post_quant_conv = nn.Conv2d(latent_channels, 256, kernel_size=1)\n",
        "\n",
        "        self.decoder_attn = SelfAttention(256)\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1),            # 128 x 32 x 32\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),    # 64 x 64 x 64\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, input_channels, kernel_size=4, stride=2, padding=1), # 3 x 128 x 128\n",
        "            nn.Sigmoid() # Predict within value range [0, 1]\n",
        "        )\n",
        "\n",
        "    def preprocess(self, x):\n",
        "        return x\n",
        "\n",
        "    def vae_encode(self, x):\n",
        "        x = self.preprocess(x)\n",
        "        h = self.encoder(x)\n",
        "        h = self.attn(h)\n",
        "        h = self.quant_conv(h)\n",
        "        mean, logvar = torch.chunk(h, 2, dim=1)\n",
        "        if self.training:\n",
        "            logvar = logvar.clamp(-30.0, 20.0)\n",
        "            std = torch.exp(0.5 * logvar)\n",
        "            eps = torch.randn_like(std)\n",
        "            z = mean + eps * std\n",
        "        else:\n",
        "            z = mean\n",
        "        return z, mean, logvar\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = self.preprocess(x)\n",
        "        h = self.encoder(x)\n",
        "        h = self.attn(h)\n",
        "        h = self.quant_conv(h)\n",
        "        mean, logvar = torch.chunk(h, 2, dim=1)\n",
        "        if self.training:\n",
        "            logvar = logvar.clamp(-30.0, 20.0)\n",
        "            std = torch.exp(0.5 * logvar)\n",
        "            eps = torch.randn_like(std)\n",
        "            z = mean + eps * std\n",
        "        else:\n",
        "            z = mean\n",
        "        return z\n",
        "\n",
        "    def decode(self, z):\n",
        "      h = self.post_quant_conv(z)\n",
        "      h = self.decoder_attn(h)\n",
        "      x_recon = self.decoder(h)\n",
        "      return x_recon\n",
        "\n",
        "    def forward(self, x):\n",
        "        z, mean, logvar = self.vae_encode(x)\n",
        "        x_recon = self.decode(z)\n",
        "        return x_recon, z, mean, logvar\n"
      ],
      "metadata": {
        "id": "2GVN5tAgzYGS"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self, latent_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(latent_dim * 32 * 32, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.fc(z)\n"
      ],
      "metadata": {
        "id": "m6ENN1mLIa7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Y8EY6OcnzX4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "from torch import nn\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- VGG16 Feature Extractor (for perceptual loss) ---\n",
        "vgg = models.vgg16(pretrained=True).features[:16].eval().to('cuda')\n",
        "for p in vgg.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "def perceptual_loss_fn(x_recon, x_true):\n",
        "    recon_feat = vgg(x_recon)\n",
        "    true_feat = vgg(x_true)\n",
        "    return F.mse_loss(recon_feat, true_feat)\n",
        "\n",
        "# --- ConvVAE Model Definition ---\n",
        "class ConvVAE(nn.Module):\n",
        "    def __init__(self, input_channels=3, latent_channels=8):\n",
        "        super().__init__()\n",
        "\n",
        "        # --- Encoder ---\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, 64, kernel_size=3, stride=2, padding=1),  # 64 x 64 x 64\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),             # 128 x 32 x 32\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),            # 256 x 32 x 32\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # --- Attention ---\n",
        "        self.attn = SelfAttention(256)\n",
        "\n",
        "        # --- Quantization conv ---\n",
        "        self.quant_conv = nn.Conv2d(256, latent_channels * 2, kernel_size=1)    # 4+4 channels\n",
        "\n",
        "        # --- Decoder ---\n",
        "        self.post_quant_conv = nn.Conv2d(latent_channels, 256, kernel_size=1)\n",
        "\n",
        "        self.decoder_attn = SelfAttention(256)\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1),            # 128 x 32 x 32\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),    # 64 x 64 x 64\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, input_channels, kernel_size=4, stride=2, padding=1), # 3 x 128 x 128\n",
        "            nn.Sigmoid() # Predict within value range [0, 1]\n",
        "        )\n",
        "\n",
        "    def preprocess(self, x):\n",
        "        return x\n",
        "\n",
        "    def vae_encode(self, x):\n",
        "        x = self.preprocess(x)\n",
        "        h = self.encoder(x)\n",
        "        h = self.attn(h)\n",
        "        h = self.quant_conv(h)\n",
        "        mean, logvar = torch.chunk(h, 2, dim=1)\n",
        "        if self.training:\n",
        "            logvar = logvar.clamp(-30.0, 20.0)\n",
        "            std = torch.exp(0.5 * logvar)\n",
        "            eps = torch.randn_like(std)\n",
        "            z = mean + eps * std\n",
        "        else:\n",
        "            z = mean\n",
        "        return z, mean, logvar\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = self.preprocess(x)\n",
        "        h = self.encoder(x)\n",
        "        h = self.attn(h)\n",
        "        h = self.quant_conv(h)\n",
        "        mean, logvar = torch.chunk(h, 2, dim=1)\n",
        "        if self.training:\n",
        "            logvar = logvar.clamp(-30.0, 20.0)\n",
        "            std = torch.exp(0.5 * logvar)\n",
        "            eps = torch.randn_like(std)\n",
        "            z = mean + eps * std\n",
        "        else:\n",
        "            z = mean\n",
        "        return z\n",
        "\n",
        "    def decode(self, z):\n",
        "      h = self.post_quant_conv(z)\n",
        "      h = self.decoder_attn(h)\n",
        "      x_recon = self.decoder(h)\n",
        "      return x_recon\n",
        "\n",
        "    def forward(self, x):\n",
        "        z, mean, logvar = self.vae_encode(x)\n",
        "        x_recon = self.decode(z)\n",
        "        return x_recon, z, mean, logvar\n",
        "\n",
        "# ----- Loss Function -----\n",
        "def vae_loss(x, x_recon, mean, logvar, vgg, kl_weight=0.1, perceptual_weight=0.1):\n",
        "    recon_loss = F.mse_loss(x, x_recon, reduction='mean')\n",
        "    kl_loss = -0.5 * torch.mean(1 + logvar - mean.pow(2) - logvar.exp())\n",
        "\n",
        "    # 计算 perceptual loss\n",
        "    perceptual_loss = perceptual_loss_fn(x_recon, x)\n",
        "\n",
        "    # 总损失\n",
        "    final_loss = recon_loss + kl_weight * kl_loss + perceptual_weight * perceptual_loss\n",
        "    return final_loss, recon_loss, kl_loss, perceptual_loss\n",
        "\n",
        "# ----- Training -----\n",
        "def train_vae(model, dataloader, optimizer, device, vgg, num_epochs=1, perceptual_weight=0.1):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        loop = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        for images, _ in loop:\n",
        "            x = images.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            x_recon, z, mean, logvar = model(x)\n",
        "            loss, recon_loss, kl_loss, perceptual_loss = vae_loss(x, x_recon, mean, logvar, vgg, perceptual_weight=perceptual_weight)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loop.set_postfix(loss=loss.item(), recon=recon_loss.item(), kl=kl_loss.item(), perceptual=perceptual_loss.item())\n",
        "         # 更新学习率\n",
        "\n",
        "# ---- Setup ----\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"T4 GPU\")\n",
        "\n",
        "# Model\n",
        "model = ConvVAE().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Train\n",
        "train_vae(model, train_loader, optimizer, device, vgg, num_epochs=15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4UqpYA5XUj6",
        "outputId": "ef977075-8d40-42f4-cc61-f11acfbfb9af"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/15: 100%|██████████| 591/591 [02:00<00:00,  4.90it/s, kl=0.239, loss=0.183, perceptual=1.5, recon=0.00905]\n",
            "Epoch 2/15: 100%|██████████| 591/591 [02:00<00:00,  4.89it/s, kl=0.249, loss=0.129, perceptual=0.978, recon=0.0059]\n",
            "Epoch 3/15: 100%|██████████| 591/591 [02:00<00:00,  4.89it/s, kl=0.273, loss=0.124, perceptual=0.911, recon=0.00543]\n",
            "Epoch 4/15: 100%|██████████| 591/591 [02:00<00:00,  4.89it/s, kl=0.316, loss=0.127, perceptual=0.89, recon=0.00629]\n",
            "Epoch 5/15: 100%|██████████| 591/591 [02:00<00:00,  4.89it/s, kl=0.345, loss=0.129, perceptual=0.887, recon=0.00548]\n",
            "Epoch 6/15: 100%|██████████| 591/591 [02:00<00:00,  4.89it/s, kl=0.33, loss=0.122, perceptual=0.834, recon=0.00583]\n",
            "Epoch 7/15: 100%|██████████| 591/591 [02:00<00:00,  4.90it/s, kl=0.322, loss=0.0988, perceptual=0.624, recon=0.00425]\n",
            "Epoch 8/15: 100%|██████████| 591/591 [02:00<00:00,  4.89it/s, kl=0.367, loss=0.103, perceptual=0.621, recon=0.00437]\n",
            "Epoch 9/15: 100%|██████████| 591/591 [02:00<00:00,  4.90it/s, kl=0.408, loss=0.112, perceptual=0.662, recon=0.00501]\n",
            "Epoch 10/15: 100%|██████████| 591/591 [02:00<00:00,  4.90it/s, kl=0.388, loss=0.107, perceptual=0.64, recon=0.00463]\n",
            "Epoch 11/15: 100%|██████████| 591/591 [02:00<00:00,  4.89it/s, kl=0.36, loss=0.0917, perceptual=0.524, recon=0.00342]\n",
            "Epoch 12/15: 100%|██████████| 591/591 [02:00<00:00,  4.90it/s, kl=0.399, loss=0.101, perceptual=0.574, recon=0.0042]\n",
            "Epoch 13/15: 100%|██████████| 591/591 [02:00<00:00,  4.89it/s, kl=0.419, loss=0.0999, perceptual=0.541, recon=0.00394]\n",
            "Epoch 14/15: 100%|██████████| 591/591 [02:00<00:00,  4.89it/s, kl=0.361, loss=0.0833, perceptual=0.445, recon=0.00276]\n",
            "Epoch 15/15: 100%|██████████| 591/591 [02:01<00:00,  4.88it/s, kl=0.415, loss=0.1, perceptual=0.543, recon=0.00442]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwks6s7kuBpu",
        "outputId": "a9acd177-f14e-4668-b9f6-1b3a17c46acc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20: 100%|██████████| 591/591 [01:00<00:00,  9.81it/s, kl=0.0604, loss=0.0182, recon=0.0121]\n",
            "Epoch 2/20: 100%|██████████| 591/591 [01:01<00:00,  9.64it/s, kl=0.0561, loss=0.0149, recon=0.00928]\n",
            "Epoch 3/20: 100%|██████████| 591/591 [01:01<00:00,  9.64it/s, kl=0.0664, loss=0.0186, recon=0.012]\n",
            "Epoch 4/20: 100%|██████████| 591/591 [01:01<00:00,  9.66it/s, kl=0.0696, loss=0.0188, recon=0.0118]\n",
            "Epoch 5/20: 100%|██████████| 591/591 [01:01<00:00,  9.64it/s, kl=0.0664, loss=0.0166, recon=0.00999]\n",
            "Epoch 6/20: 100%|██████████| 591/591 [01:01<00:00,  9.65it/s, kl=0.0535, loss=0.0129, recon=0.00756]\n",
            "Epoch 7/20: 100%|██████████| 591/591 [01:01<00:00,  9.66it/s, kl=0.0629, loss=0.0157, recon=0.00941]\n",
            "Epoch 8/20: 100%|██████████| 591/591 [01:01<00:00,  9.65it/s, kl=0.065, loss=0.0159, recon=0.00939]\n",
            "Epoch 9/20: 100%|██████████| 591/591 [01:01<00:00,  9.65it/s, kl=0.0682, loss=0.0175, recon=0.0107]\n",
            "Epoch 10/20: 100%|██████████| 591/591 [01:01<00:00,  9.63it/s, kl=0.0651, loss=0.0153, recon=0.00882]\n",
            "Epoch 11/20: 100%|██████████| 591/591 [01:01<00:00,  9.65it/s, kl=0.0599, loss=0.0149, recon=0.0089]\n",
            "Epoch 12/20: 100%|██████████| 591/591 [01:01<00:00,  9.60it/s, kl=0.0748, loss=0.0189, recon=0.0114]\n",
            "Epoch 13/20: 100%|██████████| 591/591 [01:01<00:00,  9.61it/s, kl=0.0736, loss=0.0177, recon=0.0104]\n",
            "Epoch 14/20: 100%|██████████| 591/591 [01:01<00:00,  9.60it/s, kl=0.0664, loss=0.016, recon=0.00935]\n",
            "Epoch 15/20: 100%|██████████| 591/591 [01:01<00:00,  9.59it/s, kl=0.0581, loss=0.0139, recon=0.00805]\n",
            "Epoch 16/20: 100%|██████████| 591/591 [01:01<00:00,  9.60it/s, kl=0.064, loss=0.0149, recon=0.00848]\n",
            "Epoch 17/20: 100%|██████████| 591/591 [01:01<00:00,  9.60it/s, kl=0.0824, loss=0.0194, recon=0.0111]\n",
            "Epoch 18/20: 100%|██████████| 591/591 [01:01<00:00,  9.60it/s, kl=0.0709, loss=0.0167, recon=0.00966]\n",
            "Epoch 19/20: 100%|██████████| 591/591 [01:01<00:00,  9.59it/s, kl=0.064, loss=0.0149, recon=0.00851]\n",
            "Epoch 20/20: 100%|██████████| 591/591 [01:01<00:00,  9.60it/s, kl=0.0565, loss=0.0131, recon=0.00748]\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "# 在初始化外部写一次\n",
        "import torchvision.models as models\n",
        "vgg = models.vgg16(pretrained=True).features[:16].eval().to('T4 GPU')\n",
        "for p in vgg.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "def perceptual_loss_fn(x_recon, x_true):\n",
        "    recon_feat = vgg(x_recon)\n",
        "    true_feat = vgg(x_true)\n",
        "    return F.mse_loss(recon_feat, true_feat)\n",
        "# ----- Loss Function -----\n",
        "def vae_loss(x, x_recon, mean, logvar, kl_weight=0.1):\n",
        "    recon_loss = F.mse_loss(x, x_recon, reduction='mean')\n",
        "    kl_loss = -0.5 * torch.mean(1 + logvar - mean.pow(2) - logvar.exp())\n",
        "    final_loss = recon_loss + kl_weight * kl_loss\n",
        "    return final_loss, recon_loss, kl_loss\n",
        "\n",
        "# ----- Training -----\n",
        "def train_vae(model, dataloader, optimizer, device, num_epochs=1):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        loop = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        # loop = dataloader\n",
        "        for images, labels in loop:\n",
        "            x, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            x_recon, z, mean, logvar = model(x)\n",
        "            loss, recon_loss, kl_loss = vae_loss(x, x_recon, mean, logvar)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loop.set_postfix(loss=loss.item(), recon=recon_loss.item(), kl=kl_loss.item())\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"T4 GPU\")\n",
        "\n",
        "# Model\n",
        "model = ConvVAE().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Train\n",
        "train_vae(model,train_loader, optimizer, device, num_epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "LpE4965usFxA",
        "outputId": "bd461a28-52ae-4c88-f8ec-5be0ac34987e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-68ccdd3edde0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mplot_reconstructions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_images\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "# ----- Visualization -----\n",
        "def plot_reconstructions(model, dataloader, device, num_images=8):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        x = next(iter(dataloader))[0].to(device)\n",
        "        x_recon, z, _, _ = model(x)\n",
        "        x = x.cpu().numpy()\n",
        "        x_recon = x_recon.cpu().numpy()\n",
        "        print(f\"Latent bottleneck dimension: {z.flatten(start_dim=1).shape[1]}\")\n",
        "        print(x_recon[0, :, 64, 64])\n",
        "\n",
        "        plt.figure(figsize=(16, 4))\n",
        "        for i in range(num_images):\n",
        "            # Original\n",
        "            plt.subplot(2, num_images, i+1)\n",
        "            plt.imshow(x[i].transpose(1, 2, 0))  # (C, H, W) -> (H, W, C)\n",
        "            plt.axis('off')\n",
        "\n",
        "            # Reconstruction\n",
        "            plt.subplot(2, num_images, i+1+num_images)\n",
        "            plt.imshow(x_recon[i].transpose(1, 2, 0))\n",
        "            plt.axis('off')\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "plot_reconstructions(model, train_loader, device, num_images=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftewwFD7pJzW",
        "outputId": "724704f3-2d34-451c-c0c7-d807c73fe5b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import torch\n",
            "import torch.nn as nn\n",
            "import torch.nn.functional as F\n",
            "\n",
            "class Model(nn.Module):\n",
            "    def __init__(self, input_channels=3, latent_channels=4):\n",
            "        super().__init__()\n",
            "        # Make sure the layers are consistent with your checkpoint weights\n",
            "        self.encoder = nn.Sequential(\n",
            "            nn.Conv2d(input_channels, 64, kernel_size=3, stride=2, padding=1),\n",
            "            nn.ReLU(),\n",
            "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
            "            nn.ReLU(),\n",
            "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
            "            nn.ReLU()\n",
            "        )\n",
            "        self.quant_conv = nn.Conv2d(256, latent_channels * 2, kernel_size=1)\n",
            "        self.post_quant_conv = nn.Conv2d(latent_channels, 256, kernel_size=1)\n",
            "        self.decoder = nn.Sequential(\n",
            "            nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1),\n",
            "            nn.ReLU(),\n",
            "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
            "            nn.ReLU(),\n",
            "            nn.ConvTranspose2d(64, input_channels, kernel_size=4, stride=2, padding=1),\n",
            "            nn.Sigmoid()\n",
            "        )\n",
            "\n",
            "    def preprocess(self, x):\n",
            "        # The input data is always on value range [0, 1], you may do normalization here\n",
            "        return 2 * x - 1\n",
            "\n",
            "    def encode(self, x):\n",
            "        x = self.preprocess(x)\n",
            "        h = self.encoder(x)\n",
            "        h = self.quant_conv(h)\n",
            "        mean, logvar = torch.chunk(h, 2, dim=1)\n",
            "        # Make sure you only return the latent tensor, not a tuple\n",
            "        return mean\n",
            "\n",
            "    def decode(self, z):\n",
            "        h = self.post_quant_conv(z)\n",
            "        x_recon = self.decoder(h)\n",
            "        # Make sure you only return the recon tensor, not a tuple\n",
            "        return x_recon\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Submission\n",
        "\n",
        "# 1) Save model weights\n",
        "torch.save(model.state_dict(), \"checkpoint6.pt\")\n",
        "\n",
        "# 2) Prepare the 'Model' class for submission\n",
        "with open(\"/content/drive/MyDrive/ml-com/model.py\", \"r\") as f:\n",
        "    print(f.read())\n",
        "\n",
        "# 3) Submit the model code & weights online"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDHim-KstRoc"
      },
      "outputs": [],
      "source": [
        "# Metric 1: Recon MSE on test set (on value range [0, 1])\n",
        "# Metric 2: Classification accuracy (linear probing with test set latents, 170 classes)\n",
        "# Final Score: recon_mse / probing_accuracy (the lower the better)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import importlib\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def load_model(model_path, weights_path):\n",
        "    spec = importlib.util.spec_from_file_location(\"model_module\", model_path)\n",
        "    model_module = importlib.util.module_from_spec(spec)\n",
        "    spec.loader.exec_module(model_module)  # Load the module\n",
        "    model = model_module.Model()  # Create an instance of the model class\n",
        "    model.load_state_dict(torch.load(weights_path))  # Load weights\n",
        "    print(\"model loaded successfully\")\n",
        "\n",
        "    # try small data on cpu to check if the model is loaded correctly\n",
        "    test_data = np.random.rand(3, 3, 128, 128)  # Example input data\n",
        "    test_data = torch.tensor(test_data, dtype=torch.float32)\n",
        "    test_data = test_data.to(\"cpu\")\n",
        "    model = model.to(\"cpu\")\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.encode(test_data)\n",
        "        output = model.decode(output)\n",
        "    print(\"Model loaded successfully and output generated.\")"
      ],
      "metadata": {
        "id": "M-jCYR60pqbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "# model_e and model_d are two identical instances of your model class\n",
        "def run_inference_AE(test_data_numpy, test_label_numpy, num_classes,\n",
        "                  model_e, model_d, gpu_index,\n",
        "                  batch_size=64, timeout=50, bottleNeckDim = 8192):\n",
        "    device = torch.device(f\"cuda:{gpu_index}\")\n",
        "    print(f\"Using device: {device}\")\n",
        "    model_e.to(device)  # Move the model to the GPU\n",
        "    model_e.eval()  # Set the model to evaluation mode\n",
        "    model_d.to(device)  # Move the model to the GPU\n",
        "    model_d.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    # build test dataloader from the numpy array\n",
        "    test_data = torch.tensor(test_data_numpy, dtype=torch.float32)\n",
        "    test_labels = torch.tensor(test_label_numpy, dtype=torch.long)\n",
        "    test_dataset = torch.utils.data.TensorDataset(test_data, test_labels)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "    all_latents = []\n",
        "    criterion = nn.MSELoss(reduction='sum')\n",
        "    reconstruction_loss = 0\n",
        "    shape_checked = False\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (images, labels) in enumerate(test_loader, start=1):\n",
        "            images, labels = images.to(device), labels.to(device)  # Move data to the GPU\n",
        "            latents = model_e.encode(images)\n",
        "\n",
        "            print(\"latents shape:\", latents.shape)\n",
        "            # check latents shape not too large\n",
        "            if not shape_checked:\n",
        "                latents_orig_shape = latents.shape\n",
        "                latents = latents.view(latents.shape[0], -1)\n",
        "                if latents.shape[1] > bottleNeckDim:\n",
        "                    raise ValueError(f\"Latents shape is too large: {latents.shape}. Expected less than {bottleNeckDim}.\")\n",
        "                latents = latents.view(latents_orig_shape)\n",
        "                shape_checked = True\n",
        "\n",
        "            outputs = model_d.decode(latents)\n",
        "\n",
        "            # compute reconstruction loss\n",
        "            loss = criterion(outputs, images)\n",
        "            reconstruction_loss += loss.item()\n",
        "\n",
        "            all_latents.append(latents.cpu().numpy())\n",
        "\n",
        "        reconstruction_loss = reconstruction_loss / len(test_loader.dataset)\n",
        "\n",
        "        # sample images from the latent space\n",
        "        # mean and std of all_latents\n",
        "        all_latents = np.concatenate(all_latents, axis=0)\n",
        "        mean_latents = np.mean(all_latents, axis=0)\n",
        "        std_latents = np.std(all_latents, axis=0)\n",
        "\n",
        "        # sample 5 random latents\n",
        "        random_latents = np.random.normal(mean_latents, std_latents, (all_latents[:5].shape))\n",
        "        # reconstruct the images from the latents\n",
        "        random_latents = torch.tensor(random_latents, dtype=torch.float32).to(device)\n",
        "        sampled_images = model_d.decode(random_latents)\n",
        "        sampled_images = sampled_images.cpu().numpy()\n",
        "        # save the reconstructed images, optional\n",
        "\n",
        "    # release gpu memory\n",
        "    torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "n61cfbN1pqLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EJMlkYRopqBA"
      }
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}